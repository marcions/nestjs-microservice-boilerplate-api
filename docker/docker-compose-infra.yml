version: '3'

services:
  mongo:
    container_name: mongo
    image: mongo:latest
    #restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: admin
      MONGO_INITDB_USERNAME: admin
      MONGO_INITDB_PASSWORD: admin
      MONGO_INITDB_DATABASE: db
    ports:
      - '27017:27017'
    volumes:
      - mongo-data:/data/db
      - ./mongo/mongo-init.sh:/docker-entrypoint/initdb.d/mongo-init.sh
      # - ./mongo/rs-init.sh:/scripts/mongo/rs-init.sh
    command: --auth
    networks:
      - internal

  mongo-express:
    container_name: mongo-express
    image: mongo-express
    ##restart: always
    ports:
      - '8081:8081'
    #environment:
    # ME_CONFIG_MONGODB_SERVER: mongo
    # ME_CONFIG_MONGODB_PORT: 27017
    #ME_CONFIG_MONGODB_ADMINUSERNAME: root
    #ME_CONFIG_MONGODB_ADMINPASSWORD: admin
    #ME_CONFIG_BASICAUTH_USERNAME: admin
    #ME_CONFIG_BASICAUTH_PASSWORD: admin
    #ME_CONFIG_MONGODB_URL: mongodb://root:admin@mongo:27017/?authSource=admin
    env_file:
      - ../.env
    links:
      - mongo
    depends_on:
      - mongo
    networks:
      - internal

  postgres:
    container_name: postgres
    image: postgres:latest
    #restart: always
    env_file:
      - ../.env
    ports:
      - '5432:5432'
    volumes:
      - postgres-data:/data/postgres
      - ./postgres/create-database.sql:/docker-entrypoint-initdb.d/create-database.sql
    networks:
      - internal

  pgadmin:
    container_name: pgadmin
    image: dpage/pgadmin4
    #restart: always
    env_file:
      - ../.env
    ports:
      - '16543:80'
    depends_on:
      - postgres
    networks:
      - internal

  adminer:
    container_name: adminer
    image: adminer:latest
    #restart: always
    environment:
      ADMINER_DEFAULT_DB_DRIVER: pgsql
      ADMINER_DEFAULT_DB_HOST: postgres
      ADMINER_DEFAULT_DB_NAME: db
      ADMINER_DEFAULT_SERVER: postgres
    ports:
      - 8080:8080
    depends_on:
      - postgres
    links:
      - postgres
    networks:
      - internal

  mysql:
    container_name: mysql
    image: mysql:8
    #restart: always
    ports:
      - '3306:3306'
    volumes:
      - mysql-data:/var/lib/mysql
    environment:
      MYSQL_ROOT_PASSWORD: admin
      MYSQL_DATABASE: db
      MYSQL_USER: admin
      MYSQL_PASSWORD: admin
    networks:
      - internal

  phpmyadmin:
    image: phpmyadmin/phpmyadmin:latest
    container_name: phpmyadmin
    #restart: always
    ports:
      - 8181:80
    environment:
      PMA_ARBITRARY: 1
      PMA_HOST: mysql
      PMA_PORT: 3306
      MEMORY_LIMIT: 1024M
      UPLOAD_LIMIT: 100M
      MAX_EXECUTION_TIME: 600
      MYSQL_ROOT_PASSWORD: admin
      MYSQL_DATABASE: db
    depends_on:
      - mysql
    networks:
      - internal

  redis:
    container_name: redis
    image: 'redis:alpine'
    #restart: always
    volumes:
      - redis-data:/data
    ports:
      - 6379:6379
    networks:
      - internal

  redis-commander:
    container_name: redis-commander
    image: rediscommander/redis-commander:latest
    #restart: always
    environment:
      REDIS_HOSTS: local:redis:6379
    ports:
      - 8082:8081
    networks:
      - internal

  jaeger:
    container_name: jaeger
    image: jaegertracing/all-in-one:latest
    #restart: always
    environment:
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
    ports:
      - '14250:14250'
      - '14268:14268'
      - '6831:6831/udp'
      - '16686:16686'
      - '16685:16685'
    networks:
      - internal

  zipkin:
    container_name: zipkin
    image: openzipkin/zipkin:latest
    #restart: always
    ports:
      - '9411:9411'
    env_file:
      - ../.env
    networks:
      - internal

  collector:
    container_name: collector
    image: otel/opentelemetry-collector-contrib:latest
    #restart: always
    command: ['--config=/conf/collector-config.yaml']
    volumes:
      - ./prometheus/collector-config.yaml:/conf/collector-config.yaml
    ports:
      - '9464:9464'
      - '4317:4317'
      - '4318:4318'
    depends_on:
      - zipkin
    networks:
      - internal

  prometheus:
    container_name: prometheus
    image: prom/prometheus:latest
    #restart: always
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--enable-feature=otlp-write-receiver'
      - '--enable-feature=exemplar-storage'
    env_file:
      - ../.env
    ports:
      - '9090:9090'
    volumes:
      - ./prometheus/config.yml:/etc/prometheus/prometheus.yml
    networks:
      - internal

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:management
    #restart: always
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    environment:
      RABBITMQ_ERLANG_COOKIE: 'SWQOKODSQALRPCLNMEQG'
      RABBITMQ_DEFAULT_USER: 'admin'
      RABBITMQ_DEFAULT_PASS: 'admin'
      RABBITMQ_DEFAULT_VHOST: '/'
    ports:
      - '5672:5672'
      - '15672:15672'
    networks:
      - internal

  nginx:
    container_name: nginx
    image: nginx:latest
    #restart: always
    tty: true
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./nginx/ssl:/etc/nginx/ssl
    ports:
      - '80:80'
      - '443:443'
    networks:
      - internal

  sonarqube:
    container_name: sonarqube
    image: sonarqube:community
    #restart: always
    ports:
      - '9000:9000'
    environment:
      SONAR_JDBC_URL: jdbc:postgresql://postgres:5432/sonarqube
      SONAR_JDBC_USERNAME: admin
      SONAR_JDBC_PASSWORD: admin
    volumes:
      - sonarqube-data:/opt/sonarqube/data
      #- sonarqube_extensions:/opt/sonarqube/extensions
      #- sonarqube_logs:/opt/sonarqube/logs
      #- sonarqube_temp:/opt/sonarqube/temp
    depends_on:
      - postgres
    networks:
      - internal

  kong-migrations:
    container_name: kong-migrations
    image: kong:latest
    ##restart: on-failure
    command: kong migrations bootstrap
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: postgres
      KONG_PG_DATABASE: kong
      KONG_PG_USER: admin
      KONG_PG_PASSWORD: admin
    depends_on:
      - postgres
    networks:
      - internal

  kong-migrations-up:
    container_name: kong-migrations-up
    image: kong:latest
    ##restart: on-failure
    command: kong migrations up && kong migrations finish
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: postgres
      KONG_PG_DATABASE: kong
      KONG_PG_USER: admin
      KONG_PG_PASSWORD: admin
    depends_on:
      - postgres
    networks:
      - internal

  kong:
    container_name: kong
    #build:
    #  context: ./kong
    #  dockerfile: Dockerfile
    image: kong:latest
    #restart: on-failure
    user: kong
    ports:
      - '8002:8000'
      - '8001:8001'
      - '8443:8443'
      - '8444:8444'
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: postgres
      KONG_PG_PORT: 5432
      KONG_PG_DATABASE: kong
      KONG_PG_USER: admin
      KONG_PG_PASSWORD: admin
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_PROXY_LISTEN: 0.0.0.0:8000, 0.0.0.0:8443 ssl
      KONG_ADMIN_LISTEN: 0.0.0.0:8001, 0.0.0.0:8444 ssl
      #KONG_PLUGINS: bundled,oidc
      KONG_PLUGINS: bundled
      KONG_LOG_LEVEL: debug
    healthcheck:
      test: ['CMD', 'kong', 'health']
      interval: 10s
      timeout: 10s
      retries: 10
    depends_on:
      - postgres
    networks:
      - internal

  konga-prepare:
    container_name: konga-prepare
    image: pantsel/konga:latest
    ##restart: on-failure
    command: '-c prepare -a postgres -u postgresql://admin:admin@postgres:5432/konga'
    links:
      - postgres
    depends_on:
      - postgres
    networks:
      - internal

  konga:
    container_name: konga
    image: pantsel/konga:latest
    #restart: always
    ports:
      - '1337:1337'
    environment:
      TOKEN_SECRET: km1GUr4RkcQD7DewhJPNXrCuZwcKmqjb
      DB_URI: postgresql://admin:admin@postgres:5432/konga
      NODE_ENV: development
      #NODE_ENV: production
    depends_on:
      - postgres
    networks:
      - internal

  vault:
    image: hashicorp/vault:latest
    container_name: vault
    #restart: always
    ports:
      - '8201:8201'
    environment:
      VAULT_ADDR: 'https://0.0.0.0:8201'
      VAULT_LOCAL_CONFIG: '{"listener": [{"tcp":{"address": "0.0.0.0:8201","tls_disable":"0", "tls_cert_file":"/data/vault-volume/certificate.pem", "tls_key_file":"/data/vault-volume/key.pem"}}], "default_lease_ttl": "168h", "max_lease_ttl": "720h"}, "ui": true}'
      VAULT_DEV_ROOT_TOKEN_ID: 'vault'
      VAULT_TOKEN: 'vault'
    cap_add:
      - IPC_LOCK
    volumes:
      - vault-data:/data
    healthcheck:
      retries: 5
    command: server -dev -dev-root-token-id="vault"
    networks:
      - internal

  sentry:
    image: sentry
    container_name: sentry
    #restart: always
    environment:
      SENTRY_SECRET_KEY: 'sentry'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_PASSWORD: sentry
      SENTRY_REDIS_HOST: redis
    ports:
      - 9900:9000
    links:
      - redis
      - postgres
    networks:
      - internal

  cron:
    image: sentry
    container_name: cron
    #restart: always
    command: 'sentry run cron'
    environment:
      SENTRY_SECRET_KEY: 'sentry'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_PASSWORD: sentry
      SENTRY_REDIS_HOST: redis
    links:
      - redis
      - postgres
    networks:
      - internal

  worker:
    image: sentry
    container_name: worker
    #restart: always
    command: 'sentry run worker'
    environment:
      SENTRY_SECRET_KEY: 'sentry'
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_PASSWORD: sentry
      SENTRY_REDIS_HOST: redis
    links:
      - redis
      - postgres
    networks:
      - internal

  zookeeper:
    image: bitnami/zookeeper:3.7.1
    container_name: signoz-zookeeper
    hostname: zookeeper
    user: root
    ports:
      - '2181:2181'
      - '2888:2888'
      - '3888:3888'
    volumes:
      - zookeeper-data:/bitnami/zookeeper
    environment:
      - ZOO_SERVER_ID=1
      - ZOO_SERVERS=0.0.0.0:2888:3888
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_AUTOPURGE_INTERVAL=1

  clickhouse:
    image: clickhouse/clickhouse-server:24.1.2-alpine
    container_name: signoz-clickhouse
    hostname: clickhouse
    ports:
      - '9990:9000'
      - '8123:8123'
      - '9181:9181'
    volumes:
      - ./clickhouse/clickhouse-config.xml:/etc/clickhouse-server/config.xml
      - ./clickhouse/clickhouse-users.xml:/etc/clickhouse-server/users.xml
      - ./clickhouse/clickhouse-cluster.xml:/etc/clickhouse-server/config.d/cluster.xml
      #- ./clickhouse/clickhouse-storage.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./clickhouse/custom-function.xml:/etc/clickhouse-server/custom-function.xml
      - ./clickhouse/user_scripts:/var/lib/clickhouse/user_scripts/
      - clickhouse-data:/var/lib/clickhouse/
    #restart: on-failure
    tty: true
    depends_on:
      - zookeeper
    logging:
      options:
        max-size: 50m
        max-file: '3'
    healthcheck:
      # "clickhouse", "client", "-u ${CLICKHOUSE_USER}", "--password ${CLICKHOUSE_PASSWORD}", "-q 'SELECT 1'"
      test: ['CMD', 'wget', '--spider', '-q', 'localhost:8123/ping']
      interval: 30s
      timeout: 5s
      retries: 3
    ulimits:
      nproc: 65535
      nofile:
        soft: 262144
        hard: 262144

  alertmanager:
    image: signoz/alertmanager:0.23.4
    container_name: signoz-alertmanager
    volumes:
      - alertmanager-data:/data
    depends_on:
      query-service:
        condition: service_healthy
    restart: on-failure
    command:
      - --queryService.url=http://query-service:8085
      - --storage.path=/data

  query-service:
    image: signoz/query-service:0.38.0
    container_name: signoz-query-service
    command: [
        '-config=/root/config/prometheus.yml'
        # "--prefer-delta=true"
      ]
    # ports:
    #   - "6060:6060"     # pprof port
    #   - "8080:8080"     # query-service port
    volumes:
      - ./query-service/prometheus.yml:/root/config/prometheus.yml
      - signoz-data:/var/lib/signoz/
      # - ../dashboards:/root/config/dashboards
    environment:
      - ClickHouseUrl=tcp://clickhouse:9000/?database=signoz_traces
      - ALERTMANAGER_API_PREFIX=http://alertmanager:9093/api/
      - SIGNOZ_LOCAL_DB_PATH=/var/lib/signoz/signoz.db
      - DASHBOARDS_PATH=/root/config/dashboards
      - STORAGE=clickhouse
      - GODEBUG=netdns=go
      - TELEMETRY_ENABLED=true
      - DEPLOYMENT_TYPE=docker-standalone-amd
    restart: on-failure
    healthcheck:
      test: ['CMD', 'wget', '--spider', '-q', 'localhost:8080/api/v1/health']
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      clickhouse:
        condition: service_healthy
      otel-collector-migrator:
        condition: service_completed_successfully

  frontend:
    image: signoz/frontend:0.38.0
    container_name: signoz-frontend
    restart: on-failure
    depends_on:
      - alertmanager
      - query-service
    ports:
      - '3301:3301'
    volumes:
      - ./frontend/nginx-config.conf:/etc/nginx/conf.d/default.conf

  otel-collector-migrator:
    image: signoz/signoz-schema-migrator:0.88.9
    container_name: signoz-otel-migrator
    command:
      - '--dsn=tcp://clickhouse:9000'
    depends_on:
      clickhouse:
        condition: service_healthy

  otel-collector:
    image: signoz/signoz-otel-collector:0.88.9
    container_name: signoz-otel-collector
    command:
      [
        '--config=/etc/otel-collector-config.yaml',
        '--manager-config=/etc/manager-config.yaml',
        '--copy-path=/var/tmp/collector-config.yaml',
        '--feature-gates=-pkg.translator.prometheus.NormalizeName'
      ]
    user: root # required for reading docker container logs
    volumes:
      - ./otel-collector/otel-collector-config.yaml:/etc/otel-collector-config.yaml
      - ./otel-collector/otel-collector-opamp-config.yaml:/etc/manager-config.yaml
      # - /var/lib/docker/containers:/var/lib/docker/containers:ro
    environment:
      - OTEL_RESOURCE_ATTRIBUTES=host.name=signoz-host,os.type=linux
      - DOCKER_MULTI_NODE_CLUSTER=false
      - LOW_CARDINAL_EXCEPTION_GROUPING=false
    ports:
      # - "1777:1777"     # pprof extension
      - '4317:4317' # OTLP gRPC receiver
      - '4318:4318' # OTLP HTTP receiver
      # - "8888:8888"     # OtelCollector internal metrics
      # - "8889:8889"     # signoz spanmetrics exposed by the agent
      # - "9411:9411"     # Zipkin port
      # - "13133:13133"   # health check extension
      # - "14250:14250"   # Jaeger gRPC
      # - "14268:14268"   # Jaeger thrift HTTP
      # - "55678:55678"   # OpenCensus receiver
      # - "55679:55679"   # zPages extension
    restart: on-failure
    depends_on:
      clickhouse:
        condition: service_healthy
      otel-collector-migrator:
        condition: service_completed_successfully
      query-service:
        condition: service_healthy

  logspout:
    image: 'gliderlabs/logspout:v3.2.14'
    container_name: signoz-logspout
    # volumes:
    # - /etc/hostname:/etc/host_hostname:ro
    # - /var/run/docker.sock:/var/run/docker.sock
    command: syslog+tcp://otel-collector:2255
    depends_on:
      - otel-collector
    restart: on-failure

  hotrod:
    image: jaegertracing/example-hotrod:1.30
    container_name: hotrod
    logging:
      options:
        max-size: 50m
        max-file: '3'
    command: ['all']
    environment:
      - JAEGER_ENDPOINT=http://otel-collector:14268/api/traces

  load-hotrod:
    image: 'signoz/locust:1.2.3'
    container_name: load-hotrod
    hostname: load-hotrod
    environment:
      ATTACKED_HOST: http://hotrod:8080
      LOCUST_MODE: standalone
      NO_PROXY: standalone
      TASK_DELAY_FROM: 5
      TASK_DELAY_TO: 30
      QUIET_MODE: '${QUIET_MODE:-false}'
      LOCUST_OPTS: '--headless -u 10 -r 1'
    volumes:
      - ../common/locust-scripts:/locust

volumes:
  postgres-data:
    external: true
  redis-data:
    external: true
  mongo-data:
    external: true
  rabbitmq-data:
    external: true
  mysql-data:
    external: true
  sonarqube-data:
    external: true
  vault-data:
    external: true
  zookeeper-data:
    external: true
  clickhouse-data:
    external: true
  alertmanager-data:
    external: true
  signoz-data:
    external: true

networks:
  internal:
    driver: bridge
    external: true
